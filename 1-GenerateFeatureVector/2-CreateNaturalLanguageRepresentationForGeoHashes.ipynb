{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Description2Vector for Geohashes\n",
    "\n",
    "This notebook create a description to vector representation for each geohash location R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import pygeohash as gh\n",
    "from haversine import haversine\n",
    "import time\n",
    "import cPickle\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "geohash_prec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {'LosAngeles': [33.700615, 34.353627, -118.683511, -118.074559], \n",
    "           'Houston': [29.497907,30.129003,-95.797178,-94.988191],\n",
    "           'Austin': [30.079327, 30.596764,-97.968881,-97.504838],\n",
    "           'Dallas': [32.559567,33.083278,-97.036586,-96.428928],\n",
    "           'Charlotte': [34.970168,35.423667,-81.060925,-80.622687],\n",
    "           'Atlanta': [33.612410,33.916999,-84.575600,-84.231911],\n",
    "           'Miami': [25.664776,25.942874,-80.386562,-80.118637]}\n",
    "\n",
    "time_zones = {'Houston':'US/Central', 'Charlotte':'US/Eastern', 'Miami': 'US/Eastern', 'Dallas':'US/Central',\n",
    "              'Atlanta':'US/Eastern', 'Austin':'US/Central', 'LosAngeles':'US/Pacific'}\n",
    "\n",
    "# A time interval of length 1 year, to be used to generate description to vector for each geographical region (or geohash) \n",
    "start = datetime(2017, 5, 1)\n",
    "finish   = datetime(2018, 5, 31)\n",
    "\n",
    "begin = datetime.strptime('2017-05-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end   = datetime.strptime('2018-05-31 23:59:59', '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Load Past Traffic Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq = pd.read_csv('TrafficWeatherEvent_Aug16_June19_Publish.csv') # this is the latest version of LSTW dataset\n",
    "# get the data from https://smoosavi.org/datasets/lstw\n",
    "mq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq['StartTime(UTC)'] = mq['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n",
    "mq['EndTime(UTC)'] = mq['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cities:\n",
    "    crds = cities[c]\n",
    "    subset_all = mq[(mq['Source'] == 'T') & (mq['StartTime(UTC)'] >= start) & (mq['StartTime(UTC)'] < end) & \n",
    "                    (mq['LocationLat']>crds[0]) & (mq['LocationLat']<crds[1]) & (mq['LocationLng']>crds[2]) & \n",
    "                    (mq['LocationLng']<crds[3])] \n",
    "    \n",
    "    subset_all.to_csv('data/temporary_for_nlp/MQ_{}_20170501_20180531.csv'.format(c), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load GloVe Word Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('data/glove.6B.100d.txt', 'r') as reader: # suppose that we already downloaded this GloVe model. \n",
    "    # you can download this file from https://nlp.stanford.edu/projects/glove/\n",
    "    for line in reader:\n",
    "        parts = line.replace('\\r', '').replace('\\n', '').split(' ')\n",
    "        v = [float(parts[i]) for i in range(1, len(parts))]\n",
    "        word2vec[parts[0]] = v\n",
    "        \n",
    "\n",
    "print 'loaded {} word vectors!'.format(len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_desc2vec(input):\n",
    "    parts = re.split(' - | |\\.|\\\\\\|/|;|,|&|!|\\?|\\(|\\)|\\[|\\]|\\{|\\}', input)\n",
    "    parts = [p.lower() for p in parts]\n",
    "    v = []\n",
    "    for p in parts:\n",
    "        if len(p) ==0: continue\n",
    "        if p in word2vec: v.append(word2vec[p])\n",
    "#         else: \n",
    "#             v.append(word2vec['UNK'])\n",
    "    if len(v) ==0: print input\n",
    "    v = np.mean(v, axis=0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Use Traffic Event Data to Create Embedding Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid geohashes\n",
    "valid_geohashes = set() # we only generate data for those regions/geohashes that have valid POI data \n",
    "with open('data/geohash_to_poi_vec.csv', 'r') as reader:\n",
    "    for line in reader:\n",
    "        if 'Geohash' in line: continue\n",
    "        valid_geohashes.add(line.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_to_vec = {}\n",
    "start_timestamp = time.time()\n",
    "\n",
    "for c in cities:\n",
    "    \n",
    "    # add map-quest data\n",
    "    with open('data/temporary_for_nlp/MQ_{}_20170501_20180531.csv'.format(c), 'r') as file:\n",
    "        header = False\n",
    "        for line in file:\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "                        \n",
    "            start_gh = gh.encode(float(parts[9]), float(parts[10]), precision=geohash_prec)     \n",
    "            if start_gh not in valid_geohashes: continue\n",
    "            \n",
    "            mat = []\n",
    "            if start_gh in geo_to_vec: mat = geo_to_vec[start_gh]\n",
    "            mat.append(return_desc2vec(parts[17]))\n",
    "            geo_to_vec[start_gh] = mat            \n",
    "\n",
    "    \n",
    "    print 'Done with {} in {:.1f} sec!'.format(c,time.time()-start_timestamp)\n",
    "    start_timestamp = time.time()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Create and Dump Textual Feature Vector for each Geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = open('data/geohash_to_text_vec.csv', 'w')\n",
    "writer.write('Geohash,vec\\n')\n",
    "\n",
    "for g in geo_to_vec:\n",
    "    vec = list(np.mean(geo_to_vec[g], axis=0))\n",
    "    v = [str(vec[i]) for i in range(len(vec))]\n",
    "    v = ' '.join(v)\n",
    "    writer.write(g + ',' + v + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Conda 5.2) [python/2.7 ]",
   "language": "python",
   "name": "sys_python27conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
