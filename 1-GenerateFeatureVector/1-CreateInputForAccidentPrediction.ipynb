{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is the first step toward generating input to be used for accident prediction task \n",
    "\n",
    "\n",
    "\n",
    "__Main Steps__\n",
    "\n",
    "\n",
    "* _Load traffic event data_: traffic events were largely taken from https://smoosavi.org/datasets/lstw\n",
    "\n",
    "\n",
    "* _Construct Feature Vectors for pairs of City-Geohash_: this would be an initial feature vector that only contains time, traffic, and weather information \n",
    "\n",
    "\n",
    "* _Write Primary Feature Vectors for Each Geo-hash_: next we write the primary feature vectors into file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import pygeohash as gh\n",
    "from haversine import haversine\n",
    "import time\n",
    "import cPickle\n",
    "import glob\n",
    "import json\n",
    "\n",
    "geohash_prec = 5 # the geo-hash level to define a region R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A helper class to utilize weather data for feature vector construction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weather:\n",
    "    date = ''\n",
    "    temp = 0.0\n",
    "    windchill = 0.0\n",
    "    humid = 0.0\n",
    "    pressure= 0.0\n",
    "    visib = 0.0\n",
    "    windspeed = 0.0\n",
    "    winddir = ''\n",
    "    precipitation = 0.0\n",
    "    events = ''\n",
    "    condition = ''\n",
    "    \n",
    "    def __init__(self, date, temp, windchill, humid, pressure, visib, windspeed, winddir, \n",
    "                 precipitation, events, condition, zone):\n",
    "        self.date = datetime.strptime(date, '%Y-%m-%d %I:%M:%S %p')\n",
    "        self.date = self.date.replace(tzinfo=pytz.timezone(zone))\n",
    "        self.temp = float(temp)\n",
    "        self.windchill = float(windchill)\n",
    "        self.humid = float(humid)\n",
    "        self.pressure = float(pressure)\n",
    "        self.visib = float(visib)\n",
    "        self.windspeed = float(windspeed)\n",
    "        self.winddir = winddir\n",
    "        self.precipitation = float(precipitation)\n",
    "        self.events = events\n",
    "        self.condition = condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some meta data for each city including geo-fence and time-zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {'LosAngeles': [33.700615, 34.353627, -118.683511, -118.074559], \n",
    "           'Houston': [29.497907,30.129003,-95.797178,-94.988191],\n",
    "           'Austin': [30.079327, 30.596764,-97.968881,-97.504838],\n",
    "           'Dallas': [32.559567,33.083278,-97.036586,-96.428928],\n",
    "           'Charlotte': [34.970168,35.423667,-81.060925,-80.622687],\n",
    "           'Atlanta': [33.612410,33.916999,-84.575600,-84.231911]}\n",
    "\n",
    "time_zones = {'Houston':'US/Central', 'Charlotte':'US/Eastern', 'Dallas':'US/Central',\n",
    "              'Atlanta':'US/Eastern', 'Austin':'US/Central', 'LosAngeles':'US/Pacific'}\n",
    "\n",
    "# time interval to sample data for \n",
    "start = datetime(2018, 6, 1)\n",
    "finish   = datetime(2018, 9, 2)\n",
    "\n",
    "begin = datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end   = datetime.strptime('2018-08-31 23:59:59', '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Sample Traffic Event Data for all Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq = pd.read_csv('TrafficWeatherEvent_Aug16_June19_Publish.csv')\n",
    "mq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq['StartTime(UTC)'] = mq['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n",
    "mq['EndTime(UTC)'] = mq['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cities:\n",
    "    crds = cities[c]\n",
    "    subset_all = mq[(mq['Source'] == 'T') & (mq['StartTime(UTC)'] >= start) & (mq['StartTime(UTC)'] < end) & \n",
    "                    (mq['LocationLat']>crds[0]) & (mq['LocationLat']<crds[1]) & (mq['LocationLng']>crds[2]) & \n",
    "                    (mq['LocationLng']<crds[3])]\n",
    "    \n",
    "    subset_accidents = mq[(mq['Type']=='Accident') & (mq['StartTime(UTC)'] >= start) & (mq['StartTime(UTC)'] < finish) \n",
    "                          & (mq['LocationLat']>crds[0]) & (mq['LocationLat']<crds[1]) & (mq['LocationLng']>crds[2]) \n",
    "                          & (mq['LocationLng']<crds[3])]\n",
    "    \n",
    "    print 'For {} we have {} incidents, with {} accidents! ratio {:.2f}'.format(c, len(subset_all), len(subset_accidents), \n",
    "                                                                               len(subset_accidents)*1.0/len(subset_all))\n",
    "    \n",
    "    subset_all.to_csv('temporary/MQ_{}_20180601_20180609.csv'.format(c), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set UTC as the default timezone for each traffic event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting time\n",
    "path = 'temporary/'\n",
    "mq_city2incidents = {}\n",
    "for c in cities:\n",
    "    incidents = []\n",
    "    z = time_zones[c]\n",
    "    \n",
    "    with open(path + 'MQ_{}_20180601_20180609.csv'.format(c), 'r') as file:\n",
    "        header = False\n",
    "        for line in file:\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            \n",
    "            ds = datetime.strptime(parts[6].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            ds = ds.replace(tzinfo=pytz.utc)\n",
    "            ds = ds.astimezone(pytz.timezone(z))\n",
    "            \n",
    "            de = datetime.strptime(parts[7].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            de = de.replace(tzinfo=pytz.utc)\n",
    "            de = de.astimezone(pytz.timezone(z))\n",
    "            \n",
    "            v = [parts[0], parts[2], float(parts[9]), float(parts[10]), ds, de]            \n",
    "            incidents.append(v)\n",
    "            \n",
    "    mq_city2incidents[c] = incidents\n",
    "    print ('MQ', c, len(incidents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Construct Feature Vectors for pairs of City-Geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_to_be = {}\n",
    "\n",
    "for z in ['US/Eastern', 'US/Central', 'US/Mountain', 'US/Pacific']:\n",
    "    t_begin = begin.replace(tzinfo=pytz.timezone(z))\n",
    "    t_end   = end.replace(tzinfo=pytz.timezone(z))\n",
    "    zone_to_be[z] = [t_begin, t_end]\n",
    "\n",
    "name_conversion = {'Broken-Vehicle':'BrokenVehicle', 'Flow-Incident': 'FlowIncident', 'Lane-Blocked':'RoadBlocked'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_interval_index(time_stamp, start, end):\n",
    "    if time_stamp < start or time_stamp>end: \n",
    "        return -1\n",
    "    index = int(((time_stamp - start).days*24*60 + (time_stamp-start).seconds/60)/15)\n",
    "    return index\n",
    "\n",
    "diff = int(((end - begin).days*24*60 + (end-begin).seconds/60)/15) # total_minutes/15 ==> number of 15 minutes intervals\n",
    "\n",
    "path = 'Accidents/temporary/'\n",
    "city_to_geohashes = {}\n",
    "for c in cities: city_to_geohashes[c] = {}\n",
    "\n",
    "start_timestamp = time.time()\n",
    "ccnntt = 0\n",
    "\n",
    "geocode_to_airport = {}\n",
    "aiport_to_timezone = {}\n",
    "\n",
    "for c in cities:\n",
    "    z = time_zones[c]\n",
    "    \n",
    "    # add map-quest data\n",
    "    with open(path + 'MQ_{}_20180601_20180609.csv'.format(c), 'r') as file:\n",
    "        header = False\n",
    "        for line in file:\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            \n",
    "            ds = datetime.strptime(parts[11].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            ds = ds.replace(tzinfo=pytz.utc)\n",
    "            ds = ds.astimezone(pytz.timezone(z))\n",
    "            s_interval = return_interval_index(ds, zone_to_be[z][0], zone_to_be[z][1])\n",
    "            if s_interval==-1: continue\n",
    "                \n",
    "            de = datetime.strptime(parts[12].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            de = de.replace(tzinfo=pytz.utc)\n",
    "            de = de.astimezone(pytz.timezone(z))\n",
    "            e_interval = return_interval_index(de, zone_to_be[z][0], zone_to_be[z][1])\n",
    "            if e_interval == -1: e_interval = diff-1    \n",
    "            \n",
    "            start_gh = gh.encode(float(parts[6]), float(parts[7]), precision=geohash_prec)\n",
    "            intervals = []\n",
    "            if start_gh not in city_to_geohashes[c]:\n",
    "                for i in range(diff): \n",
    "                    intervals.append({'Construction':0, 'Congestion':0, 'Accident':0, 'FlowIncident':0, 'Event':0, \n",
    "                                      'BrokenVehicle':0, 'RoadBlocked':0, 'Other':0})\n",
    "            else:\n",
    "                intervals = city_to_geohashes[c][start_gh]\n",
    "            \n",
    "            if parts[4] in name_conversion:\n",
    "                tp = name_conversion[parts[4]]\n",
    "            else: \n",
    "                tp = parts[4].split('-')[0]\n",
    "                \n",
    "            for i in range(s_interval, e_interval+1):                \n",
    "                v = intervals[i]\n",
    "                if tp in v: v[tp] = v[tp] + 1\n",
    "                else: v['Other'] = v['Other'] + 1\n",
    "                intervals[i] = v\n",
    "                \n",
    "                if tp == 'Accident': break # this means counting number of accidents only once! does this make sense?\n",
    "                \n",
    "            city_to_geohashes[c][start_gh] = intervals\n",
    "            \n",
    "            ap = parts[27]\n",
    "            if len(ap) > 3:\n",
    "                if start_gh not in geocode_to_airport:\n",
    "                    geocode_to_airport[start_gh] = set([ap])\n",
    "                else:\n",
    "                    st = geocode_to_airport[start_gh]\n",
    "                    st.add(ap)\n",
    "                    geocode_to_airport[start_gh] = st\n",
    "                aiport_to_timezone[ap] = z\n",
    "  \n",
    "    \n",
    "    print 'Done with {} in {:.1f} sec! there are {} geohashes with data!'.format(c, \n",
    "                                time.time()-start_timestamp, len(city_to_geohashes[c]))\n",
    "    start_timestamp = time.time()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and sort relevant weather data\n",
    "airports_to_observations = {}\n",
    "for g in geocode_to_airport:\n",
    "    aps = geocode_to_airport[g]\n",
    "    for a in aps:\n",
    "        if a not in airports_to_observations:\n",
    "            airports_to_observations[a] = []\n",
    "\n",
    "print '{} airports to collect data for!'.format(len(airports_to_observations))\n",
    "            \n",
    "w_path = 'Sample_Weather/' # this directory contains weather observation records for each airport\n",
    "airport_to_data = {}\n",
    "for ap in airports_to_observations:\n",
    "    data = []\n",
    "    z = aiport_to_timezone[ap]\n",
    "    print 'Airport {}'.format(ap)\n",
    "    header = ''\n",
    "    with open(w_path + ap + '.csv', 'r') as file:\n",
    "        for line in file:\n",
    "            if 'Airport' in line: \n",
    "                header = line.replace('\\r','').replace('\\n','').replace(',Hour','')\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            try:\n",
    "                w = weather(parts[1] + ' ' + parts[2].split(' ')[0] + ':00 ' + parts[2].split(' ')[1], parts[3], parts[4], \n",
    "                           parts[5], parts[6], parts[7], parts[8], parts[9], parts[10], parts[11], parts[12], z)   \n",
    "                data.append(w)\n",
    "            except:\n",
    "                continue\n",
    "    data.sort(key=lambda x:x.date)\n",
    "    airport_to_data[ap] = data\n",
    "    \n",
    "print '\\nData for {} airport stations is loaded!'.format(len(airport_to_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find missing Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in city_to_geohashes:\n",
    "    for g in city_to_geohashes[c]:\n",
    "        if g not in geocode_to_airport:\n",
    "            gc = gh.decode_exactly(g)[0:2]\n",
    "            min_dist = 1000000000\n",
    "            close_g = ''\n",
    "            for _g in geocode_to_airport:\n",
    "                _gc = gh.decode_exactly(_g)[0:2]\n",
    "                dst = haversine(gc, _gc, 'km')\n",
    "                if dst < min_dist:\n",
    "                    min_dist = dst\n",
    "                    close_g = _g\n",
    "#             print g, close_g, min_dist\n",
    "            geocode_to_airport[g] = geocode_to_airport[close_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_to_geohashes_to_weather = {}\n",
    "\n",
    "for c in city_to_geohashes:\n",
    "    start = time.time()\n",
    "    geo2weather = {}\n",
    "    for g in city_to_geohashes[c]:\n",
    "        w_data = []\n",
    "        for i in range(len(city_to_geohashes[c][g])):\n",
    "            w_data.append({'Temperature':[], 'Humidity':[], 'Pressure':[], 'Visibility':[], 'WindSpeed':[], \n",
    "                          'Precipitation':[], 'Condition':set(), 'Event':set()})\n",
    "        # populate weather data\n",
    "        aps = geocode_to_airport[g]\n",
    "        for a in aps:\n",
    "            z = aiport_to_timezone[a]\n",
    "            a_w_data = airport_to_data[a]\n",
    "            prev = 0\n",
    "            for a_w_d in a_w_data:\n",
    "                idx = return_interval_index(a_w_d.date, zone_to_be[z][0], zone_to_be[z][1])\n",
    "                if idx >-1:\n",
    "                    for i in range(prev, min(idx+1, len(w_data))):\n",
    "                        _w = w_data[i]\n",
    "                        \n",
    "                        _tmp = _w['Temperature']\n",
    "                        if a_w_d.temp > -1000:\n",
    "                            _tmp.append(a_w_d.temp)\n",
    "                            _w['Temperature'] = _tmp\n",
    "                        \n",
    "                        _hmd = _w['Humidity']\n",
    "                        if a_w_d.humid > -1000:\n",
    "                            _hmd.append(a_w_d.humid)\n",
    "                            _w['Humidity'] = _hmd\n",
    "                        \n",
    "                        _prs = _w['Pressure']\n",
    "                        if a_w_d.pressure > -1000:\n",
    "                            _prs.append(a_w_d.pressure)\n",
    "                            _w['Pressure'] = _prs\n",
    "                        \n",
    "                        _vis = _w['Visibility']\n",
    "                        if a_w_d.visib > -1000:\n",
    "                            _vis.append(a_w_d.visib)\n",
    "                            _w['Visibility'] = _vis\n",
    "                            \n",
    "                        _wspd = _w['WindSpeed']\n",
    "                        if a_w_d.windspeed > -1000:\n",
    "                            _wspd.append(a_w_d.windspeed)\n",
    "                            _w['WindSpeed'] = _wspd\n",
    "                            \n",
    "                        _precip = _w['Precipitation']\n",
    "                        if a_w_d.precipitation > -1000:\n",
    "                            _precip.append(a_w_d.precipitation)\n",
    "                            _w['Precipitation'] = _precip\n",
    "                            \n",
    "                        _cond = _w['Condition']\n",
    "                        _cond.add(a_w_d.condition)\n",
    "                        _w['Condition'] = _cond\n",
    "                        \n",
    "                        _evnt = _w['Event']\n",
    "                        _evnt.add(a_w_d.events)\n",
    "                        _w['Event'] = _evnt\n",
    "                        \n",
    "                        w_data[i] = _w\n",
    "                        \n",
    "                    prev = idx+1\n",
    "                                                \n",
    "            \n",
    "        geo2weather[g] = w_data\n",
    "    city_to_geohashes_to_weather[c] = geo2weather\n",
    "    print 'Done with {} in {:.1f} sec!'.format(c, time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Daylight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dayLight:\n",
    "    sunrise = []\n",
    "    sunset = []\n",
    "    def __init__(self, sunrise, sunset):\n",
    "        self.sunrise = sunrise\n",
    "        self.sunset = sunset\n",
    "        \n",
    "def return_time(x):\n",
    "    try:\n",
    "        h = int(x.split(':')[0])\n",
    "        m = int(x.split(':')[1].split(' ')[0])\n",
    "        if 'pm' in x and h < 12: h = h + 12\n",
    "        return [h,m]\n",
    "    except: return [0,0]\n",
    "\n",
    "    \n",
    "def returnDayLight(city, state, dt):\n",
    "    sc = city + '-' + state\n",
    "    days = city_days_time[sc]\n",
    "    d = str(dt.year) + '-' + str(dt.month) + '-' + str(dt.day)\n",
    "    if d in days:\n",
    "        r = days[d]\n",
    "        if ((dt.hour>r.sunrise[0] and dt.hour<r.sunset[0]) or\n",
    "            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<r.sunset[0]) or\n",
    "            (dt.hour>r.sunrise[0] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1]) or \n",
    "            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1])):\n",
    "            return '1'\n",
    "        else: return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_days_time = {}\n",
    "\n",
    "days = {}\n",
    "city = ''\n",
    "with open('sample_daylight.csv', 'r') as file:            \n",
    "    for ln in file.readlines():\n",
    "        parts = ln.replace('\\r','').replace('\\n','').split(',')\n",
    "\n",
    "        if parts[0] != city:\n",
    "            if len(city) > 0: \n",
    "                if city in city_days_time:\n",
    "                    _days = city_days_time[city]\n",
    "                    for _d in _days: days[_d] = _days[_d]\n",
    "                city_days_time[city] = days\n",
    "\n",
    "            city = parts[0]\n",
    "            days = {}\n",
    "\n",
    "        sunrise = return_time(parts[2])\n",
    "        sunset  = return_time(parts[3])\n",
    "        dl = dayLight(sunrise, sunset)\n",
    "        days[parts[1]] = dl\n",
    "\n",
    "if city in city_days_time:\n",
    "    _days = city_days_time[city]\n",
    "    for _d in _days: days[_d] = _days[_d]\n",
    "city_days_time[city] = days\n",
    "\n",
    "\n",
    "print 'Successfully loaded daylight data for {} cities!'.format(len(city_days_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load daylight mapping for different cities\n",
    "city_to_index_to_daylight = {}\n",
    "states = {'Houston':'TX', 'Charlotte':'NC', 'Dallas':'TX', 'Atlanta':'GA', 'Austin':'TX', 'LosAngeles':'CA'}\n",
    "for c in cities:\n",
    "    d_begin = begin.replace(tzinfo=pytz.timezone(time_zones[c]))\n",
    "    d_end   = end.replace(tzinfo=pytz.timezone(time_zones[c]))\n",
    "    index_to_daylight = {}\n",
    "    index = 0\n",
    "    while(d_begin < d_end):\n",
    "        dl = returnDayLight(c, states[c], d_begin)\n",
    "        index_to_daylight[index] = dl\n",
    "        index += 1\n",
    "        d_begin += timedelta(seconds=15*60)\n",
    "    city_to_index_to_daylight[c] = index_to_daylight\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Write Feature Vectors\n",
    "\n",
    "Here each vector represent a 15 minutes time interval for each geohash (geohash: a region of size 5km x 5km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each time-step to hour of day and day of the week; this should be consistent across different time-zones!\n",
    "timestep_to_dow_hod = {}\n",
    "d_begin = begin.replace(tzinfo=pytz.utc)\n",
    "d_end   = end.replace(tzinfo=pytz.utc)\n",
    "index = 0\n",
    "\n",
    "while(d_begin < d_end):\n",
    "    dow = d_begin.weekday()\n",
    "    hod = d_begin.hour    \n",
    "    timestep_to_dow_hod[index] = [dow, hod]\n",
    "    \n",
    "    d_begin += timedelta(seconds=15*60)    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traffic_tags = ['Accident', 'BrokenVehicle', 'Congestion', 'Construction', 'Event', 'FlowIncident', 'Other', 'RoadBlocked']\n",
    "weather_tags = ['Condition', 'Event', 'Humidity', 'Precipitation', 'Pressure', 'Temperature', 'Visibility', 'WindSpeed']\n",
    "poi_tags = []\n",
    "start = time.time()\n",
    "condition_tags = set()\n",
    "\n",
    "for c in city_to_geohashes:\n",
    "    writer = open('vectors/{}_geo2vec_{}-{}.csv'.format(c, str(begin.year)+str(begin.month)+str(begin.day),\n",
    "                                                        str(end.year)+str(end.month)+str(end.day)), 'w')\n",
    "    writer.write('Geohash,TimeStep,DOW,HOD,DayLight,T-Accident,T-BrokenVehicle,T-Congestion,T-Construction,'\\\n",
    "        'T-Event,T-FlowIncident,T-Other,T-RoadBlocked,W-Humidity,W-Precipitation,W-Pressure,'\\\n",
    "        'W-Temperature,W-Visibility,W-WindSpeed,W-Rain,W-Snow,W-Fog,W-Hail\\n')\n",
    "    \n",
    "    traffic = city_to_geohashes[c]\n",
    "    weather = city_to_geohashes_to_weather[c]        \n",
    "    for g in traffic:\n",
    "        vectors = []\n",
    "        for i in range(len(traffic[g])):\n",
    "            v = []\n",
    "            for t in traffic_tags: v.append(traffic[g][i][t])\n",
    "            v_w = [0,0,0,0] # for rain, snow, fog, and hail\n",
    "            for w in weather_tags:\n",
    "                if w=='Condition' or w=='Event':      \n",
    "                    _tgs = weather[g][i][w]\n",
    "                    for _tg in _tgs: \n",
    "                        if 'rain' in _tg.lower() or 'drizzle' in _tg.lower() or 'thunderstorm' in _tg.lower(): v_w[0] = 1\n",
    "                        elif 'snow' in _tg.lower(): v_w[1] = 1\n",
    "                        elif 'fog' in _tg.lower() or 'haze' in _tg.lower() or 'mist' in _tg.lower() or 'smoke' in _tg.lower(): v_w[2] = 1\n",
    "                        elif 'hail' in _tg.lower() or 'ice pellets' in _tg.lower(): v_w[3] = 1                            \n",
    "                elif len(weather[g][i][w]) == 0: v.append(0)\n",
    "                else: v.append(np.mean(weather[g][i][w]))\n",
    "            for _v_w in v_w: v.append(_v_w)\n",
    "            vectors.append(v)\n",
    "        \n",
    "        for i in range(len(vectors)):\n",
    "            v = vectors[i]\n",
    "            v = [str(v[j]) for j in range(len(v))]\n",
    "            v = ','.join(v)\n",
    "            writer.write(g + ',' + str(i) + ',' + str(timestep_to_dow_hod[i][0]) + ',' + str(timestep_to_dow_hod[i][1]) \n",
    "                         + ',' + city_to_index_to_daylight[c][i] + ',' + v + '\\n')\n",
    "            \n",
    "    writer.close()\n",
    "    print 'Done with {} in {:.1f} sec! #vectors {}!'.format(c, time.time()-start, len(traffic)*len(vectors))\n",
    "    start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Conda 5.2) [python/2.7 ]",
   "language": "python",
   "name": "sys_python27conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
