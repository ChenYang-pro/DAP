{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Accident Prediction Model (DAP)\n",
    "\n",
    "This notebook contains a Keras implementation of our Deep Accident Prediction model (DAP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,LSTM,Activation,Dropout,BatchNormalization,Input,Embedding\n",
    "from keras.layers import Flatten,Conv2D,MaxPooling2D,Bidirectional,concatenate\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import keras_metrics as km\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from multiprocessing import cpu_count,Pool \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import recall_score,precision_score,f1_score,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from utils import *\n",
    "from base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 2\n",
    "dropout=0.2\n",
    "VAL_SPLIT = 0.2\n",
    "patience = 15\n",
    "lr=0.01\n",
    "weight_decay = 0.0000\n",
    "lr_decay=1e-6\n",
    "ADD_ON_LAYERS = True\n",
    "ACT_PRIOR = 'sigmoid'\n",
    "ACT_POSTERIOR = 'relu'\n",
    "LSTM_UNIT = 128\n",
    "GEOHASH_UNIT = 128\n",
    "EMBEDDING_UNIT = 128\n",
    "Embedding_outdim = 128\n",
    "NLP_UNIT = 128\n",
    "SEQ_UNIT = 256\n",
    "DENSE_CONCAT = 512\n",
    "CONV_UNIT = 32\n",
    "weights = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAP(keras_model):\n",
    "    \n",
    "    def load_data(self):\n",
    "        super(DAP,self).load_data(with_geocode=True)\n",
    "        \n",
    "        self.X_train1 = self.reshape(self.X_train[:,:-1])\n",
    "        self.X_test1 = self.reshape(self.X_test[:,:-1])\n",
    "        \n",
    "        self.X_train2 = reshape_cat(self.X_train[:,:-1],'geohash') # geohash indicates POI attributes \n",
    "        self.X_train3 = reshape_cat(self.X_train[:,:-1],'NLP') # NLP indicates Desc2Vec attributes\n",
    "        \n",
    "        self.X_test2 = reshape_cat(self.X_test[:,:-1],'geohash')\n",
    "        self.X_test3 = reshape_cat(self.X_test[:,:-1],'NLP')\n",
    "        \n",
    "        self.X_train4 = self.X_train[:,-1]\n",
    "        self.X_test4 = self.X_test[:,-1]\n",
    "        \n",
    "        print (self.X_train1.shape)\n",
    "        print (self.X_train2.shape)\n",
    "        print (self.X_train3.shape)\n",
    "        print (self.X_train4.shape)\n",
    "        \n",
    "    def create_model(self):\n",
    "        \n",
    "        input1 = Input(shape=(self.X_train1.shape[1], self.X_train1.shape[2]),dtype='float32', \n",
    "                           name='main_input')\n",
    "        lstm = LSTM(units = LSTM_UNIT, return_sequences = True,\n",
    "                     kernel_regularizer=regularizers.l2(self.weight_decay),\n",
    "                     recurrent_regularizer = regularizers.l2(self.weight_decay),\n",
    "                     dropout=dropout,\n",
    "                     recurrent_dropout=dropout,\n",
    "                     unroll = True)(input1)\n",
    "        \n",
    "        lstm = LSTM(units = LSTM_UNIT, return_sequences = False,\n",
    "                     kernel_regularizer=regularizers.l2(self.weight_decay),\n",
    "                     recurrent_regularizer = regularizers.l2(self.weight_decay),\n",
    "                     dropout=dropout,\n",
    "                     recurrent_dropout=dropout,\n",
    "                     unroll = True)(lstm)\n",
    "        ######################################\n",
    "        input2 = Input(shape=(self.X_train2.shape[1],), dtype='float32', name='geohash_input')\n",
    "        geohash_vec = Dense(GEOHASH_UNIT, activation=ACT_PRIOR)(input2)\n",
    "        ######################################\n",
    "        input3 = Input(shape=(self.X_train3.shape[1],), dtype='float32', name='nlp_input')\n",
    "        nlp_vec = Dense(NLP_UNIT, activation=ACT_PRIOR)(input3)\n",
    "        ######################################\n",
    "        input4 = Input(shape=(1,),dtype='int32',name='geo_code')\n",
    "        embeding = Embedding(input_dim=935, output_dim=Embedding_outdim, embeddings_initializer='uniform',input_length=1)(input4)\n",
    "        embeding = Flatten()(embeding)\n",
    "        embeding = Dense(EMBEDDING_UNIT, activation=ACT_PRIOR)(embeding)\n",
    "        ######################################\n",
    "        level_3 = concatenate([lstm,geohash_vec,nlp_vec,embeding])\n",
    "        \n",
    "        main_output = self.last_layers(level_3)\n",
    "        \n",
    "        self.model = Model(inputs=[input1,input2,input3,input4], outputs=main_output)\n",
    "        \n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train(self):\n",
    "        history = self.model.fit([self.X_train1,self.X_train2,self.X_train3,self.X_train4], self.y_train, batch_size=self.batch_size, \n",
    "                                 epochs=self.epoch,verbose=verbose,validation_split=VAL_SPLIT, callbacks=[self.earlyStopping])        \n",
    "    def evaluate(self):    \n",
    "        y_true, y_pred =  self.y_test, self.model.predict([self.X_test1,self.X_test2,self.X_test3,self.X_test4],verbose=verbose)\n",
    "        return self.make_report(y_true, y_pred)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(city='Atlanta'):\n",
    "    def initialte_class():\n",
    "        mypred = DAP(city=city)\n",
    "        return mypred\n",
    "    \n",
    "    def do_rest(pred):\n",
    "        pred.load_data()\n",
    "        pred.create_model()\n",
    "        pred.compile_model()\n",
    "        pred.train()\n",
    "        return pred\n",
    "    \n",
    "    def process_frame(df,i):\n",
    "        new_df = df[['0','1','weighted avg','micro avg','macro avg']].drop('support',axis=0)\n",
    "        new_df=new_df.stack().swaplevel()\n",
    "        new_df.index=new_df.index.map('{0[0]}_{0[1]}'.format) \n",
    "        new_df = new_df.to_frame().T\n",
    "        new_df['run'] = i\n",
    "        new_df = new_df.set_index('run')\n",
    "        return new_df\n",
    "    def rerun(classname):\n",
    "        df_list=[]\n",
    "        for i in range(3):\n",
    "            print (\"*\"*20,classname,\"*\"*20)\n",
    "            print ('*'*10,' round ', i)\n",
    "            mypred = initialte_class()\n",
    "            mypred = do_rest(mypred)\n",
    "            res  = mypred.evaluate()\n",
    "            df_list.append(process_frame(res,i))\n",
    "        df = pd.concat(df_list)\n",
    "        return pd.DataFrame(df.mean(),columns=[classname])\n",
    "    \n",
    "    return rerun('DAP')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = ['Atlanta', 'Austin', 'Charlotte', 'Dallas', 'Houston', 'LosAngeles']\n",
    "for city in cities:\n",
    "    result = Train_Model(city) #the output 'result' contains prediction evaluation metrics such as f1-score "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
